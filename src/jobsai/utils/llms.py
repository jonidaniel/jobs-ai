"""
LLM Utilities - OpenAI API Integration and JSON Extraction.

This module provides utilities for interacting with OpenAI's LLM API and
processing LLM responses. It includes:

- call_llm: Central function for all LLM API calls with automatic retry logic
- extract_json: Utility for extracting JSON from LLM responses that may be
  wrapped in markdown or contain extra text

The module handles:
- Environment variable validation
- OpenAI client initialization
- Automatic retry with exponential backoff for transient failures
- Response validation and error handling
"""

import os
import logging
import json
import time
from typing import Optional

from dotenv import load_dotenv

from openai import OpenAI
from openai import RateLimitError, APIConnectionError, APITimeoutError

logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()

# Retrieve OpenAI configuration from environment variables
OPENAI_MODEL = os.getenv("OPENAI_MODEL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Validate required environment variables
# These must be set for the application to function
if not OPENAI_MODEL:
    error_msg = (
        "OPENAI_MODEL not found in environment variables. "
        "Please set OPENAI_MODEL in your .env file or environment."
    )
    logger.error(error_msg)
    raise ValueError(error_msg)

if not OPENAI_API_KEY:
    error_msg = (
        "OPENAI_API_KEY not found in environment variables. "
        "Please set OPENAI_API_KEY in your .env file or environment."
    )
    logger.error(error_msg)
    raise ValueError(error_msg)

# Initialize OpenAI client with validated API key
# This will raise an error immediately if the API key format is invalid
try:
    client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    error_msg = f"Failed to initialize OpenAI client: {str(e)}"
    logger.error(error_msg)
    raise ValueError(error_msg) from e


def call_llm(
    system_prompt: str,
    user_prompt: str,
    max_tokens: int = 800,
    max_retries: int = 3,
    retry_delay: float = 1.0,
) -> str:
    """
    Call OpenAI LLM API with system and user prompts.

    This is the central function for all LLM interactions in the JobsAI system.
    It's used by:
    - ProfilerAgent: To extract skill profiles from form submissions
    - ReporterAgent: To generate cover letter instructions
    - GeneratorAgent: To write cover letter content

    Includes automatic retry logic for transient failures (rate limits, timeouts, connection errors).

    Args:
        system_prompt (str): System prompt defining the LLM's role and behavior
        user_prompt (str): User prompt containing the actual task/input
        max_tokens (int): Maximum number of tokens in the response (default: 800)
        max_retries (int): Maximum number of retry attempts for transient failures (default: 3)
        retry_delay (float): Initial delay between retries in seconds, doubles on each retry (default: 1.0)

    Returns:
        str: The complete LLM response text

    Raises:
        Exception: If OpenAI API call fails after all retries (handled by caller)
    """

    retryable_errors = (RateLimitError, APIConnectionError, APITimeoutError)
    last_exception = None

    for attempt in range(max_retries + 1):
        try:
            # Make API call to OpenAI
            # Temperature is set low (0.2) for more deterministic, focused responses
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=max_tokens,
                temperature=0.2,  # Low temperature for consistent, focused output
            )
            break  # Success, exit retry loop

        except retryable_errors as e:
            last_exception = e
            if attempt < max_retries:
                # Exponential backoff: delay doubles with each retry
                delay = retry_delay * (2**attempt)
                logger.warning(
                    f"LLM API call failed (attempt {attempt + 1}/{max_retries + 1}): {type(e).__name__}. "
                    f"Retrying in {delay:.1f}s..."
                )
                time.sleep(delay)
            else:
                # Final attempt failed
                logger.error(
                    f"LLM API call failed after {max_retries + 1} attempts: {type(e).__name__}"
                )
                raise
        except Exception as e:
            # Non-retryable errors (e.g., authentication, invalid request) fail immediately
            logger.error(
                f"LLM API call failed with non-retryable error: {type(e).__name__}: {str(e)}"
            )
            raise

    # Validate response structure
    if not response or not hasattr(response, "choices"):
        error_msg = (
            "Invalid response structure from OpenAI API: missing 'choices' attribute"
        )
        logger.error(error_msg)
        raise ValueError(error_msg)

    if not response.choices or len(response.choices) == 0:
        error_msg = "OpenAI API returned empty choices array"
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Extract text content from response
    message = response.choices[0].message
    if not message or not hasattr(message, "content"):
        error_msg = (
            "Invalid response structure from OpenAI API: missing 'content' attribute"
        )
        logger.error(error_msg)
        raise ValueError(error_msg)

    text = message.content

    # Check if content is None (can happen with some API responses)
    if text is None:
        error_msg = "OpenAI API returned None content in response"
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Log first 500 characters for debugging (full response may be very long)
    logger.debug(" LLM response: %s", text[:500])

    return text


def extract_json(text: str) -> Optional[str]:
    """
    Extract JSON substring from raw LLM response text.

    LLMs often return JSON wrapped in markdown code blocks or with extra text.
    This function finds and extracts just the JSON portion by:
    1. Finding the first opening brace '{'
    2. Balancing braces to find the matching closing brace '}'
    3. Extracting the substring between them

    Args:
        text (str): The raw LLM response text (may contain markdown, explanations, etc.)

    Returns:
        Optional[str]:
            - The extracted JSON string if valid JSON is found
            - None if no valid JSON can be extracted

    Example:
        Input: "Here is the profile: ```json\n{\"name\": \"John\"}\n```"
        Output: '{"name": "John"}'
    """

    # Find where the JSON object starts (first opening brace)
    start = text.find("{")

    # If no opening brace found, there's no JSON
    if start == -1:
        return None

    # Balance braces to find the matching closing brace
    # This handles nested objects correctly
    brace = 0
    for i in range(start, len(text)):
        if text[i] == "{":
            brace += 1
        elif text[i] == "}":
            brace -= 1
            # When braces are balanced, we've found the complete JSON object
            if brace == 0:
                return text[start : i + 1]

    # Fallback: if brace balancing didn't work, try parsing the entire text
    # This handles cases where the text is already valid JSON
    try:
        json.loads(text)
        return text
    except Exception:
        return None
